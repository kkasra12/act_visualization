[
  {
    "objectID": "readme.html",
    "href": "readme.html",
    "title": "install dependencies",
    "section": "",
    "text": "install dependencies\nto install dependencies, run pip install -r requirements.txt\n\n\ncreate slides\nto create slides, run\njupyter-nbconvert --to slides main.ipynb \\\n --SlidesExporter.reveal_theme=serif \\\n --SlidesExporter.reveal_scroll=True \\\n --SlidesExporter.reveal_transition=concave \\\n --SlidesExporter.exclude_input=True \\\n --SlidesExporter.reveal_number=\"v.h\"\nthere should be a file named main.slides.html. open it in a browser to view the slides.\n\n\nrerun the classification\nif you want to rerun the classification: create a Python file named config.py and define API_KEY.\necho \"API_KEY = &lt;YOUR_API_KEY&gt;\" &gt; config.py\n\ndelete the classifier.pkl file or change its name. Also, you can change the file name in the code (you can find it almost end of main.ipynb).\nrun main.ipynb again.\n\n\n\nview the slides\nto see the rendered slides click here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AI Act Analysis",
    "section": "",
    "text": "Code\nimport os\nimport pickle\nimport re\nimport random\n\nimport matplotlib.lines as mlines\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import Markdown, display\nfrom matplotlib import colormaps\nfrom matplotlib.cm import ScalarMappable\nfrom matplotlib.colors import Normalize\nimport nltk\nfrom nltk import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom seaborn import objects as so\nfrom sklearn.decomposition import PCA\nfrom tqdm.notebook import tqdm\nfrom wordcloud import WordCloud\nfrom IPython.display import Javascript\n\nfrom classifier import Classifier\nfrom node import Node, node_sum_sent\nfrom scrapper import Scrapper\n\nsns.set_style(\"darkgrid\")\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('wordnet')\n\n\n[nltk_data] Downloading package stopwords to /home/kasra/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/kasra/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to /home/kasra/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "index.html#word-count",
    "href": "index.html#word-count",
    "title": "AI Act Analysis",
    "section": "word count",
    "text": "word count\nin this part we should consider the following points:\n\nwe should remove the punctuations\nwe should remove the stop words\nwe should remove the numbers\nwe should remove the words with length less than 3\n\n\n\nCode\n# to see the stop words\n\", \".join(stopwords.words(\"english\"))\n\n\n\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\"\n\n\n\n\nCode\ndef validate_word(word):\n    word = word.lower()\n    if word.isalpha() and word not in stopwords.words(\"english\"):\n        return word\n    return None\n\ngreek_to_english = [\n    \"i\",\n    \"ii\",\n    \"iii\",\n    \"iv\",\n    \"v\",\n    \"vi\",\n    \"vii\",\n    \"viii\",\n    \"ix\",\n    \"x\",\n    \"xi\",\n    \"xii\",\n    \"xiii\",\n]\n\n\n\n\nCode\nall_words = []\nlast_article = 0\nchapter_counter = []\nfor page in scrapper.content:\n    article_number = (\n        greek_to_english.index(\n            re.search(r\"Home\\s.\\sTitle(.+?)\\s\", page[0]).group(1).strip().lower()  # type: ignore\n        )\n        + 1\n    )\n    if article_number != last_article:\n        # all_words.append(f'zArticle_{article_number}')\n        print(f\"Article {article_number} has {len(all_words)} words.\")\n        chapter_counter.append(len(all_words))\n        last_article = article_number\n    for paragraph in page:\n        for word in word_tokenize(paragraph):\n            word = validate_word(word)\n            if word:\n                all_words.append(word)\n\nvalues, count = np.unique(all_words, return_counts=True)\n\n\nArticle 1 has 0 words.\nArticle 2 has 1128 words.\nArticle 3 has 1582 words.\nArticle 4 has 6154 words.\nArticle 5 has 6193 words.\nArticle 6 has 6456 words.\nArticle 7 has 6596 words.\nArticle 8 has 6600 words.\nArticle 9 has 6836 words.\nArticle 10 has 6840 words.\nArticle 11 has 7046 words.\nArticle 12 has 7054 words.\n\n\n\n\nCode\nwords_count = list(zip(*np.unique(all_words, return_counts=True)))\nwords_count.sort(key=lambda x: x[1], reverse=True)\nwords_count[:10]\n\n\n[('ai', 237),\n ('system', 229),\n ('article', 183),\n ('shall', 180),\n ('title', 161),\n ('systems', 111),\n ('iii', 106),\n ('chapter', 104),\n ('regulation', 93),\n ('home', 85)]\n\n\n\n\nCode\nlimit = 10\nfig = sns.barplot(\n    y=[w[0] for w in words_count[:limit]], x=[w[1] for w in words_count[:limit]]\n)\nfig.set(xlabel=\"Words\", ylabel=\"Frequency\", title=f\"Top {limit} Words\")\nplt.show()\n\n\n\n\n\n\n\n\n\nas we can see, there is a word \"system\" and \"systems\" in the list. we should consider them as one word. we can use stemming or lemmatization to do that. I have tried many different algorithms, but finally I find the WordNetLemmatizer the most accurate one.\n\nlemmatization\n\n\nCode\nwnl = WordNetLemmatizer()\nstemmed_words = [wnl.lemmatize(word) for word in all_words]\nvalues, count = np.unique(stemmed_words, return_counts=True)\nsorted_counts = np.argsort(count)[::-1]\nn = 5\nprint(f\"The most {n} common stemmed words are:\")\nprint(\"\\n\".join([f\"{values[i]:&gt;10s}: {count[i]: 3d} times\" for i in sorted_counts[:n]]))\n\n\nThe most 5 common stemmed words are:\n    system:  340 times\n        ai:  237 times\n   article:  187 times\n     shall:  180 times\n     title:  161 times\n\n\n\n\nCode\nstemmed_words_count_ = list(zip(*np.unique(stemmed_words, return_counts=True)))\nstemmed_words_count_.sort(key=lambda x: x[1], reverse=True)\nstemmed_words_count = pd.DataFrame(stemmed_words_count_, columns=[\"word\", \"count\"])\nstemmed_words_count\n\n\n\n\n\n\n\n\n\nword\ncount\n\n\n\n\n0\nsystem\n340\n\n\n1\nai\n237\n\n\n2\narticle\n187\n\n\n3\nshall\n180\n\n\n4\ntitle\n161\n\n\n...\n...\n...\n\n\n1163\nvulnerable\n1\n\n\n1164\nwebsite\n1\n\n\n1165\nweight\n1\n\n\n1166\nwhereas\n1\n\n\n1167\nwhose\n1\n\n\n\n\n1168 rows × 2 columns\n\n\n\n\n\nCode\nfig = sns.barplot(\n    y=stemmed_words_count.iloc[:limit, 0], x=stemmed_words_count.iloc[:limit, 1]\n)\nfig.set(xlabel=\"Words\", ylabel=\"Frequency\", title=f\"Top {limit} Words\")\nplt.show()"
  },
  {
    "objectID": "index.html#word-cloud",
    "href": "index.html#word-cloud",
    "title": "AI Act Analysis",
    "section": "Word Cloud",
    "text": "Word Cloud\n\n\nCode\nwc = WordCloud(background_color=\"white\", max_words=1000, width=800, height=400)\nwc.generate_from_frequencies(stemmed_words_count.set_index(\"word\").to_dict()[\"count\"])\nplt.figure(figsize=(12, 6))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nTODO1: I should try different lemmatisers and stemmers to see which one is better. right now we have words such \"titl\", \"chapt\", \"dat\" which sounds not real words.\nTODO2: create a word cloud for each page and see if there is any pattern in the words.\nTODO3: create words density for each page and see if there is any pattern in the words.\nTODO4: perform tf-idf on the words and see if there is any pattern in the words. (we can consider each page as a document)\nTODO5: use a pretrained word2vec model to plot the words in a 2D space.\nTODO6: create new dataset for text-completion and QA models.\nTODO:density of the some words in the document"
  },
  {
    "objectID": "index.html#glove-word2vec-embedding",
    "href": "index.html#glove-word2vec-embedding",
    "title": "AI Act Analysis",
    "section": "GloVe Word2Vec Embedding",
    "text": "GloVe Word2Vec Embedding\nThe Global Vectors for word representation (GloVe), introduced by Jeffrey Pennington et al Pennington, Socher, and Manning (2014)\n\n\nCode\nfile_path = \"glove.6B/glove.6B.50d.txt\"\n!head $file_path\n\n\nthe 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n, 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n. 0.15164 0.30177 -0.16763 0.17684 0.31719 0.33973 -0.43478 -0.31086 -0.44999 -0.29486 0.16608 0.11963 -0.41328 -0.42353 0.59868 0.28825 -0.11547 -0.041848 -0.67989 -0.25063 0.18472 0.086876 0.46582 0.015035 0.043474 -1.4671 -0.30384 -0.023441 0.30589 -0.21785 3.746 0.0042284 -0.18436 -0.46209 0.098329 -0.11907 0.23919 0.1161 0.41705 0.056763 -6.3681e-05 0.068987 0.087939 -0.10285 -0.13931 0.22314 -0.080803 -0.35652 0.016413 0.10216\nof 0.70853 0.57088 -0.4716 0.18048 0.54449 0.72603 0.18157 -0.52393 0.10381 -0.17566 0.078852 -0.36216 -0.11829 -0.83336 0.11917 -0.16605 0.061555 -0.012719 -0.56623 0.013616 0.22851 -0.14396 -0.067549 -0.38157 -0.23698 -1.7037 -0.86692 -0.26704 -0.2589 0.1767 3.8676 -0.1613 -0.13273 -0.68881 0.18444 0.0052464 -0.33874 -0.078956 0.24185 0.36576 -0.34727 0.28483 0.075693 -0.062178 -0.38988 0.22902 -0.21617 -0.22562 -0.093918 -0.80375\nto 0.68047 -0.039263 0.30186 -0.17792 0.42962 0.032246 -0.41376 0.13228 -0.29847 -0.085253 0.17118 0.22419 -0.10046 -0.43653 0.33418 0.67846 0.057204 -0.34448 -0.42785 -0.43275 0.55963 0.10032 0.18677 -0.26854 0.037334 -2.0932 0.22171 -0.39868 0.20912 -0.55725 3.8826 0.47466 -0.95658 -0.37788 0.20869 -0.32752 0.12751 0.088359 0.16351 -0.21634 -0.094375 0.018324 0.21048 -0.03088 -0.19722 0.082279 -0.09434 -0.073297 -0.064699 -0.26044\nand 0.26818 0.14346 -0.27877 0.016257 0.11384 0.69923 -0.51332 -0.47368 -0.33075 -0.13834 0.2702 0.30938 -0.45012 -0.4127 -0.09932 0.038085 0.029749 0.10076 -0.25058 -0.51818 0.34558 0.44922 0.48791 -0.080866 -0.10121 -1.3777 -0.10866 -0.23201 0.012839 -0.46508 3.8463 0.31362 0.13643 -0.52244 0.3302 0.33707 -0.35601 0.32431 0.12041 0.3512 -0.069043 0.36885 0.25168 -0.24517 0.25381 0.1367 -0.31178 -0.6321 -0.25028 -0.38097\nin 0.33042 0.24995 -0.60874 0.10923 0.036372 0.151 -0.55083 -0.074239 -0.092307 -0.32821 0.09598 -0.82269 -0.36717 -0.67009 0.42909 0.016496 -0.23573 0.12864 -1.0953 0.43334 0.57067 -0.1036 0.20422 0.078308 -0.42795 -1.7984 -0.27865 0.11954 -0.12689 0.031744 3.8631 -0.17786 -0.082434 -0.62698 0.26497 -0.057185 -0.073521 0.46103 0.30862 0.12498 -0.48609 -0.0080272 0.031184 -0.36576 -0.42699 0.42164 -0.11666 -0.50703 -0.027273 -0.53285\na 0.21705 0.46515 -0.46757 0.10082 1.0135 0.74845 -0.53104 -0.26256 0.16812 0.13182 -0.24909 -0.44185 -0.21739 0.51004 0.13448 -0.43141 -0.03123 0.20674 -0.78138 -0.20148 -0.097401 0.16088 -0.61836 -0.18504 -0.12461 -2.2526 -0.22321 0.5043 0.32257 0.15313 3.9636 -0.71365 -0.67012 0.28388 0.21738 0.14433 0.25926 0.23434 0.4274 -0.44451 0.13813 0.36973 -0.64289 0.024142 -0.039315 -0.26037 0.12017 -0.043782 0.41013 0.1796\n\" 0.25769 0.45629 -0.76974 -0.37679 0.59272 -0.063527 0.20545 -0.57385 -0.29009 -0.13662 0.32728 1.4719 -0.73681 -0.12036 0.71354 -0.46098 0.65248 0.48887 -0.51558 0.039951 -0.34307 -0.014087 0.86488 0.3546 0.7999 -1.4995 -1.8153 0.41128 0.23921 -0.43139 3.6623 -0.79834 -0.54538 0.16943 -0.82017 -0.3461 0.69495 -1.2256 -0.17992 -0.057474 0.030498 -0.39543 -0.38515 -1.0002 0.087599 -0.31009 -0.34677 -0.31438 0.75004 0.97065\n's 0.23727 0.40478 -0.20547 0.58805 0.65533 0.32867 -0.81964 -0.23236 0.27428 0.24265 0.054992 0.16296 -1.2555 -0.086437 0.44536 0.096561 -0.16519 0.058378 -0.38598 0.086977 0.0033869 0.55095 -0.77697 -0.62096 0.092948 -2.5685 -0.67739 0.10151 -0.48643 -0.057805 3.1859 -0.017554 -0.16138 0.055486 -0.25885 -0.33938 -0.19928 0.26049 0.10478 -0.55934 -0.12342 0.65961 -0.51802 -0.82995 -0.082739 0.28155 -0.423 -0.27378 -0.007901 -0.030231\n\n\n\n\nCode\npickle_file_path = \"word_embeddings.pkl\"\nif os.path.exists(pickle_file_path):\n    print(\"Loading word embeddings from pickle file\")\n    with open(pickle_file_path, \"rb\") as f:\n        word_embeddings = pickle.load(f)\nelse:\n    print(\"Loading word embeddings from txt file\")\n    word_embeddings = {}\n    limit = -1\n    with open(file_path, \"r\") as f:\n        for i, line in enumerate(f):\n            if i == limit:\n                break\n            values = line.split()\n            word = values[0]\n            vector = np.asarray(values[1:], dtype=\"float32\")\n            word_embeddings[word] = vector\n    print(\"Saving word embeddings to pickle file\")\n    with open(\"word_embeddings.pkl\", \"wb\") as f:\n        pickle.dump(word_embeddings, f)\n\nprint(f\"{len(word_embeddings)} words found in the embeddings.\")\n\n\nLoading word embeddings from pickle file\n400000 words found in the embeddings.\n\n\n\n\nCode\ntop = stemmed_words_count.iloc[:15].copy()\ntop_embeddings_ = [word_embeddings[w] for w in top.word]\ntop_embeddings: np.ndarray = np.array(top_embeddings_)\nprint(f\"Top {len(top)} words embeddings shape: {top_embeddings.shape}\")\n\n\nTop 15 words embeddings shape: (15, 50)"
  },
  {
    "objectID": "index.html#pca",
    "href": "index.html#pca",
    "title": "AI Act Analysis",
    "section": "PCA",
    "text": "PCA\nPCA is a function that mapps a vector to another vector with less dimensions.\n\\[\n\\begin{align*}\nf:& \\mathbb{R}^n \\rightarrow \\mathbb{R}^m, m &lt; n \\\\\n& x \\rightarrow Mx\n\\end{align*}\n\\]\n\n\nCode\npca = PCA(n_components=2)\ntop_embeddings_pca = pca.fit_transform(top_embeddings)\ntop[\"x\"] = top_embeddings_pca[:, 0]\ntop[\"y\"] = top_embeddings_pca[:, 1]\ntop\n\n\n\n\n\n\n\n\n\nword\ncount\nx\ny\n\n\n\n\n0\nsystem\n340\n-1.778601\n-0.804771\n\n\n1\nai\n237\n2.659739\n-0.694570\n\n\n2\narticle\n187\n0.387305\n0.015132\n\n\n3\nshall\n180\n0.919920\n2.539926\n\n\n4\ntitle\n161\n2.925060\n-1.077561\n\n\n5\niii\n106\n3.110993\n-0.372795\n\n\n6\nchapter\n104\n1.149682\n0.218735\n\n\n7\nregulation\n93\n-1.723553\n1.114024\n\n\n8\nauthority\n86\n-1.345177\n1.170021\n\n\n9\nhome\n85\n0.334574\n-2.272493\n\n\n10\nprovider\n81\n-2.255407\n-2.361256\n\n\n11\nreferred\n73\n0.673860\n-0.180351\n\n\n12\ndata\n68\n-2.859118\n-1.898228\n\n\n13\nconformity\n63\n-0.535149\n3.028187\n\n\n14\nrequirement\n63\n-1.664126\n1.576002\n\n\n\n\n\n\n\n\n\nCode\nplt.figure(figsize=(9, 9))\nax = sns.scatterplot(data=top, x=\"x\", y=\"y\", size=\"count\", sizes=(60, 400), legend=True)\nax.set_title(\"Top 10 Words Embeddings\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\", rotation=0, labelpad=10)\n\nax.vlines(0, *ax.get_xlim(), colors=\"gray\", alpha=0.6, linestyles=\"dashed\")\nax.hlines(0, *ax.get_ylim(), colors=\"gray\", alpha=0.6, linestyles=\"dashed\")\nfor i, row in top.iterrows():\n    ax.text(row.x, row.y, row.word, fontsize=12)\n\nax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ntop_norm = top.copy()\nmax_font_size = 50\nmin_font_size = 15\nmin_color = 0.5\nmax_color = 1\nc = top_norm[\"count\"]\ntop_norm[\"size\"] = (c - c.min()) / (c.max() - c.min()) * (\n    max_font_size - min_font_size\n) + min_font_size\n# top_norm.drop(columns=['count'], inplace=True)\ntop_norm\n\n\n\n\n\n\n\n\n\nword\ncount\nx\ny\nsize\n\n\n\n\n0\nsystem\n340\n-1.778601\n-0.804771\n50.000000\n\n\n1\nai\n237\n2.659739\n-0.694570\n36.985560\n\n\n2\narticle\n187\n0.387305\n0.015132\n30.667870\n\n\n3\nshall\n180\n0.919920\n2.539926\n29.783394\n\n\n4\ntitle\n161\n2.925060\n-1.077561\n27.382671\n\n\n5\niii\n106\n3.110993\n-0.372795\n20.433213\n\n\n6\nchapter\n104\n1.149682\n0.218735\n20.180505\n\n\n7\nregulation\n93\n-1.723553\n1.114024\n18.790614\n\n\n8\nauthority\n86\n-1.345177\n1.170021\n17.906137\n\n\n9\nhome\n85\n0.334574\n-2.272493\n17.779783\n\n\n10\nprovider\n81\n-2.255407\n-2.361256\n17.274368\n\n\n11\nreferred\n73\n0.673860\n-0.180351\n16.263538\n\n\n12\ndata\n68\n-2.859118\n-1.898228\n15.631769\n\n\n13\nconformity\n63\n-0.535149\n3.028187\n15.000000\n\n\n14\nrequirement\n63\n-1.664126\n1.576002\n15.000000\n\n\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(12, 12))\n\n# plt.plot(top_norm.x, top_norm.y, markersize=1, marker=\".\", linestyle=\"\", cmap=cmap, label = top_norm.word)\n\n# sns.scatterplot(data=top_norm, x='x', y='y', hue='color', sizes=(1,2))\ncolor_norm = Normalize(vmin=top_norm[\"count\"].min(), vmax=top_norm[\"count\"].max())\ncmap = colormaps[\"viridis\"]\n\nfor i, row in top_norm.iterrows():\n    color = cmap(color_norm(row[\"count\"]))\n    ax.plot(row.x, row.y, markersize=1, marker=\".\", linestyle=\"\", color=\"white\")\n    ax.text(\n        row.x,\n        row.y,\n        row.word,\n        fontsize=row[\"size\"],\n        color=color,\n        horizontalalignment=\"center\",\n        verticalalignment=\"center\",\n        alpha=0.8,\n        fontweight=\"bold\",\n    )\n\n\nxmin, xmax, ymin, ymax = ax.axis(\"equal\")\nax.vlines(0, xmin * 2, xmax * 2, colors=\"gray\", alpha=0.6, linestyles=\"dashed\")\nax.hlines(0, ymin * 2, ymax * 2, colors=\"gray\", alpha=0.6, linestyles=\"dashed\")\nax.set_xlim(xmin, xmax)\nax.set_ylim(ymin, ymax)\nax.set_title(\"Top 10 Words Embeddings\")\nax.set_xlabel(\"X\")\nax.set_ylabel(\"Y\", rotation=0, labelpad=10)\nfig.colorbar(\n    ScalarMappable(norm=color_norm, cmap=cmap),\n    ax=ax,\n    orientation=\"vertical\",\n    label=\"frequency\",\n)\nplt.show()"
  },
  {
    "objectID": "index.html#impoertant-bigrams",
    "href": "index.html#impoertant-bigrams",
    "title": "AI Act Analysis",
    "section": "Impoertant Bigrams",
    "text": "Impoertant Bigrams\n\n\n\n\n\n\nNote\n\n\n\nlets filter some of uninformative words…\n\n\n\n\nCode\nfiltered_words = {\n    \"chapter\",\n    \"title\",\n    \"home\",\n    \"iii\"}\n\nfrequent_bigrams = find_n_grams([w for w in stemmed_words if w not in filtered_words], 2)\nfrequent_bigrams = (\n    frequent_bigrams.to_frame().reset_index().rename(columns={\"index\": \"bigram\"})\n)\nfrequent_bigrams[\"bigram\"] = frequent_bigrams[\"bigram\"].str.join(\"_\")\n\nwc = WordCloud(background_color=\"white\", max_words=1000, width=800, height=400)\nwc.generate_from_frequencies(frequent_bigrams.set_index(\"bigram\").to_dict()[\"count\"])\nplt.figure(figsize=(12, 6))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"
  },
  {
    "objectID": "index.html#count-3-seccessive-words",
    "href": "index.html#count-3-seccessive-words",
    "title": "AI Act Analysis",
    "section": "Count 3 seccessive words",
    "text": "Count 3 seccessive words\n\n\nCode\nfrequent_threegrams = find_n_grams([w for w in stemmed_words if w not in filtered_words], 3)\nfrequent_threegrams = (\n    frequent_threegrams.to_frame().reset_index().rename(columns={\"index\": \"threegram\"})\n)\nfrequent_threegrams[\"threegram\"] = frequent_threegrams[\"threegram\"].str.join(\"_\")\nfrequent_threegrams.set_index(\"threegram\").to_dict()[\"count\"]\n\nwc = WordCloud(background_color=\"white\", max_words=1000, width=800, height=400)\nwc.generate_from_frequencies(\n    frequent_threegrams.set_index(\"threegram\").to_dict()[\"count\"]\n)\nplt.figure(figsize=(12, 6))\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()"
  }
]